## ISTA 450 and INFO 550 Course Project
#### *Please read the following instructions very carefully, and let me know as soon as possible if you have any questions! I am here to help.*

**Overview:** The course project is an opportunity to explore AI applications and concepts that go beyond what we will learn in class or read in the textbook. Your project should focus on comparing AI algorithms across a range of novel problems. By completing the project, you will develop the skills necessary to choose the right AI algorithm for whatever problem you are asked to solve.

**Getting Started:** To get started, please use the link in the d2l to create a GitHub repository for your code. You must create this repository before submitting your project proposal (see below). Please use [this link](https://forms.gle/4Ckw7KP4LG6Qgynb8) to register your GitHub ID, so we know whose project is whose. For those new to GitHub, please watch [this tutorial](https://www.youtube.com/watch?v=Oaj3RBIoGFc) on how to get started with Git and GitHub.

**Topic:** Selecting a topic requires: (i) choosing the AI algorithms you will use and (ii) choosing the problems you will apply them to. When making these choices, bear in mind that the primary aim of your project will be to compare the performance of a set of algorithms across a set of problems to test interesting hypotheses. A good rule of thumb is that the fewer algorithms you cover, the more problems I will expect (and vice versa).

The following rules also apply: (i) All students must select at least one algorithm that IS covered in the course. (ii) All students must select at least one problem that is NOT covered in the course. *(iii) Graduate students must also select at least one algorithm not covered in the course. Undergraduates may complete the graduate requirement for extra credit.*

- **Note:** You are not required to implement your chosen algorithms right away, so don't be afraid to choose algorithms covered later in the course. See timeline below for more details.
- **Note:** By testing "interesting" hypotheses, I mean more than predicting which algorithm will perform best on different applications. You should predict that specific algorithms will work better in specific ways for specific reasons grounded in the properties of the algorithms and applications. 

**Timeline:** The project involves three separate submissions: (1) a project proposal, (2) a progress report, and (2) a final report. There are are drop boxes for these submissions under "Assignments". Exact due dates can be found on the syllabus, but (1) will be due around the end of the first full month of the course, (2) will be due around the end of the second full month of the course, and (3) will be due at the end of the course.  
#### **Part 1, Project Proposal:**
For the project proposal, you will (i) identify the algorithms and problems you intend to cover, (ii) identify the kinds of comparisons you plan to make, (iii) explain (at least roughly) what kinds of hypotheses you might test with these comparisons. Proposals are graded on effort, but I will provide feedback about the direction, scope, and feasibility of your project. You should expect to revise your project plans in light of this feedback. Your proposal should be 1-2 double-spaced pages. 
In the sub-module below, I have included several template projects which you can use directly or use as examples of acceptable projects. If you are unsure about what you want to do, I strongly suggest just using one of the templates and filling it out with specific algorithms, problems, and other details to create your own proposal.
- **Note:** It may help to start by thinking about your project as answering a question you find interesting and choose your topic accordingly. For example, when are the computational costs of using a more complicated heuristic justified by the increased efficiency of search? In this case, you might compare an algorithm with no heuristic, a simple heuristic, and a complicated heuristic across different problem types and problem difficulty levels. You might hypothesize, for example, that the more complicated heuristic will be especially costly for a particular type of problem given the nature of the problem and of the heuristic. You can test this hypothesis by seeing whether the prediction it suggests is correct (i.e., that heuristic will take longer to compute for one kind of problem than for others).

#### **Part 2, Progress Report:**
Once you have read and incorporated my feedback on your proposal, your next goal will be to implement the code needed for your chosen problem(s). As such, your project report will consist of (i) a revised version of your proposal and (ii) a link to a GitHub repository with working problem code.
Your repository should use the file structure given in the template repository. We will run the file "demo.py" to assess your problem code. It should show a brief example of a random agent trying to solve each of your problems. The file "algorithms.py" should contain a random agent class, and the file "problems.py" should contain the code for whatever problems (e.g., games) you have chosen. Your completed project code will add agent classes to "algorithms.py" that utilize your chosen algorithms to solve your problems (e.g., a minimax agent class), but "problems.py" should require no major edits after the progress report is complete. If you are unsure about how to set up a random agent for your problem, please ask me for help.
- **Note:** You may use or modify any code I have given, and I suggest using Tic-Tac-Toe to test your algorithms (if appropriate) given its brevity and simplicity. However, it will not count as one of your problems since I have already implemented it for you.
  
#### **Part 3, Final Report:**
Your final report should (i) briefly introduce your project, (ii) concisely explain your chosen algorithms and problems focusing on the key differences between them, (iii) explain the hypotheses your project will test, (iv) describe the comparisons you will make to test those hypotheses, (v) explain what results you initially expected from those comparisons, and (vi) explain why you expected those results given your hypotheses. Next, your final report should (vi) describe the results you actually observed and (vii) try to explain any results that surprised you. You may use or modify any relevant material from your project proposal in your final report.
Your final report should be approximately 7-9 double-spaced pages in length and include a link to your GitHub repository with the final version of your project code. Code for all of your comparisons should be included in "comparisons.py," but I recommend setting it up so that any subset of those comparisons can be run as needed. You will not receive credit for any comparison not found in "comparisons.py." Your comparison code need not generate your figures, but it must generate the data itself. 
The final version of "demo.py" should show each algorithm running on each problem for a short period of time. Make sure this file is running as expected since it is the first one we will run when grading your code. 
- **Note:** Your final report will not be graded on whether you made the correct prediction. It will be graded on whether you have implemented a sufficient number of problems/algorithms, whether they are implemented correctly, and whether you have completed the elements above. You shouldn't remove incorrect predictions, but try to explain why you were mistaken. That is often more interesting than explaining why you were right! 
- **Note:** You should write your report on the assumption that I have never heard of the algorithms you are comparing. I need not be able to implement them based on your explanation, but I should understand (i) the basic idea of how they work and (ii) any differences between them that are relevant to the comparisons you plan to make or the results that you predict. 
#### **Technical:** 
You should write all of your code from scratch. This means implementing both the code for your problem and your algorithms. Please annotate your code with comments that will help me understand it. This is vital in assigning partial credit if something is wrong with your implementation. 
Your "demo.py" file should visually represent each algorithm attempting each problem, but the visual representations of the problems/solutions may be as simple as they need to be to clearly convey the relevant information (e.g., a grid of characters printed to the terminal to represent a checkerboard). 
I will run all code within a Python environment that includes only NumPy, SciPy, matplotlib, and OpenCV (for displaying images). If your code relies on any other library (except those included in Python itself), it will not receive credit. We have included instructions for setting up the evaluation environment in a sub-module below. If you use this procedure, you can be sure that your code will work on my computer. Please use this environment to test your code from the start to avoid headaches later. 
#### **Academic Integrity:**
You are not permitted to use any code that you did not personally write--whether it comes from another human being or an AI system, like ChatGPT. You may read articles on algorithms, converse with an AI system about key concepts or error messages, and gather general information from any source so long as you clearly cite it in the references of your final report (e.g., by linking directly to your AI chat log). 
You must make regular updates to your GitHub repository and describe any important changes you have made. I expect at least 10 commits before your progress report and 10 additional commits before your final report.  Failure to do so will result in (i) a significant reduction in your project grade and (ii) a careful inspection of your code for plagiarism. If plagiarism is found, the sanctions described in the syllabus will apply.
- **Note:** The best problems for this project are not those that require extremely complicated implementations, but those that are simple to implement yet present a challenging problem for AI agents. Games like snake, sudoku, or checkers are ideal since they are simple to describe but complicated to solve. If you choose problems appropriately, you should not need to rely on other people's implementations to get them right.  

### **TEMPLATES:**

#### **Search Algorithms**

This project involves creating novel variants of the A* algorithm not covered in class. Each variant will employ a customized heuristic function designed to work well with different problems. While a route planning problem may involve conventional distance metrics as heuristics (e.g., Euclidean distance or Manhattan distance), other problems require other heuristics. For example, in the 8-puzzle, what heuristic might you use to guess how many slides remain?  Remember, this heuristic must be admissible and consistent. 

With this in mind, the project will involve selecting four search problems, each of which require a different kind of heuristic. For each of these four problems, you will design two admissible and consistent heuristics, predict which of the two will work better, and compare the performance of these heuristics to each other and to the performance baseline of uniform cost search (which like A* uses the cost, but which employs no heuristic). These comparisons should be based on a representative sample of each problem type (e.g., multiple mazes or multiple shuffles of the 8-puzzle board). As part of the project, you will explain what predictions you made and why. You will also try to explain any results that contradicted your predictions. 


#### **Adversarial Search Algorithms**

This project involves comparing: (i) the performance of variable-depth minimax search with hand-designed evaluation functions, move-ordering heuristics, and stopping rules to (ii) the performance of the sampling-based adversarial search algorithm Monte Carlo tree search. The latter will provide a baseline for evaluating the quality of your hand-designed elements.

For this project, you will select two to three deterministic adversarial games (e.g., checkers, reversi, or chess) and develop an evaluation function, a move-ordering heuristic (perhaps based on your evaluation function) and a stopping rule for each of these games through a process of deliberate design as well as trial-and-error. You will describe this process in detail in your report, describing each draft of these three elements, why you thought it would work, and how its performance actually compared to previous drafts and to Monte Carlo tree search. Make sure to try and explain any results that surprised you.

The end result should be a competent AI agent for playing each of the two to three games you have selected. 


#### **Reinforcement Learning Algorithms**

This project involves comparing the performance of the Q-learning algorithm and approximate Q-learning algorithm. For the approximate Q-learning algorithm, you will develop a set of features for two to three simple and scalable games (e.g., snake with varying board size, pong with varying ball count). You will develop and refine your feature sets through testing. In particular, you will develop a large feature set, consisting of 10-20 features for each game. Before testing, you will predict which features will be most useful and why. To test these predictions, you will train an approximate Q-learning agent using these features and see which have the highest magnitude weights. You will then compare the weight magnitudes to your predictions. Were the ones you thought would be most important the most heavily weighted? If not, try to explain the discrepancy. 

Once you have a good set of features (consider dropping features with low magnitude weights), you will compare the performance of your approximate Q-learning agent to your regular Q-learning agent across your chosen games. For each game, you will compare the algorithms across different complexity levels (e.g., different snake board sizes) given a fixed training period. Approximate Q-learning can generalize better than regular Q-learning, but lacks a capacity to memorize the best moves. This means that it typically performs best when memorization becomes too difficult. Start with a simple enough version of each game so that regular Q-learning has the upper hand but loses its advantage as the game gets more complicated. Report the complexity level at which approximate Q-learning takes the lead. Make sure to use a representative sample of training runs for each game at each complexity level. 
